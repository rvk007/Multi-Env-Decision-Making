# env
env:
  config_dir: env_configs
  save_video: true
  record_frequency: 1
  names:
    - highway-v0
    - merge-v0
    - roundabout-v0

# train
num_train_steps: 300001
num_train_iters: 1
num_exploration_steps: 5000
start_training_steps: 1600
min_eps: 0.1
replay_buffer_capacity: ${num_train_steps}
# Prioritized Replay Buffer
prioritized_replay: false
prioritized_replay_alpha: 0.6
seed: 1
# eval
eval_frequency: 30000
num_eval_steps: 125000
# misc
log_frequency_step: 10000
log_save_tb: true
device: cuda
# global params
lr: 0.0001
beta_1: 0.9
beta_2: 0.999
weight_decay: 0.0
adam_eps: 0.00015
max_grad_norm: 10.0
hidden_depth: 2
batch_size: 32

# agent configuration
agent:
  name: drql
  class: drql.DRQLAgent
  params:
    obs_shape: ??? # to be specified later
    action_shape: ??? # to be specified later
    num_env_paths: ??? # to be specified later
    device: ${device}
    encoder_cfg: ${encoder}
    critic_cfg: ${critic}
    discount: 0.99
    lr: ${lr}
    beta_1: ${beta_1}
    beta_2: ${beta_2}
    weight_decay: ${weight_decay}
    adam_eps: ${adam_eps}
    max_grad_norm: ${max_grad_norm}
    critic_tau: 0.9
    critic_target_update_frequency: 1
    batch_size: ${batch_size}
    multistep_return: 10
    eval_eps: 0.05
    # Double Q learning
    double_q: true
    prioritized_replay_beta0: 0.4
    prioritized_replay_beta_steps: ${num_train_steps}

critic:
  class: drql.Critic
  params:
    encoder_cfg: ${agent.params.encoder_cfg}
    action_shape: ${agent.params.action_shape}
    num_env_paths: ${agent.params.num_env_paths}
    hidden_dim: 64
    hidden_depth: ${hidden_depth}
    # Dueling DQN
    dueling: false

encoder:
  class: drql.Encoder
  params:
    input_shape: ${agent.params.obs_shape}

experiment: base_config

# hydra configuration
hydra:
  name: double_dqn
  run:
    dir: ./exp_local/${hydra.name}_${experiment}
  sweep:
    dir: ./exp/${hydra.name}/${now:%H%M%S}_${agent.name}_${experiment}
    subdir: ${hydra.job.num}
